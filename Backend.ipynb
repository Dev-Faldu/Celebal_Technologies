import os
import sys
import warnings
warnings.filterwarnings('ignore')

!pip install fastapi
!pip install uvicorn
!pip install pyngrok
!pip install pandas
!pip install numpy
!pip install scikit-learn
!pip install shap
!pip install lime
!pip install transformers
!pip install torch
!pip install langchain
!pip install langchain-community
!pip install langchain-huggingface
!pip install pydantic
!pip install python-multipart
!pip install nest-asyncio
!pip install aiofiles
!pip install prometheus-client
!pip install psutil
!pip install plotly
!pip install seaborn
!pip install matplotlib
!pip install scipy
!pip install joblib


# ============================================================================
# üìä Core Imports & Advanced Configuration
# ============================================================================

# Core system imports
import asyncio
import uuid
import json
import logging
import datetime
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import traceback
import hashlib
import pickle
from collections import deque
import threading
import time

# Data science & ML stack
import pandas as pd
import numpy as np
from sklearn.neural_network import MLPRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import ElasticNet
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import joblib

# Advanced analytics & explainability
import shap
from lime import lime_tabular
from scipy import stats

# Deep learning & NLP
import torch
import transformers
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM

# API framework
from fastapi import FastAPI, HTTPException, BackgroundTasks, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, HTMLResponse
from pydantic import BaseModel, validator, Field
import uvicorn

# Deployment & tunneling
from pyngrok import ngrok
import nest_asyncio

# Advanced monitoring
from prometheus_client import Counter, Histogram, generate_latest
import psutil

# Visualization
import plotly.graph_objects as go
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt

# Enable nested async loops for Colab
nest_asyncio.apply()

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("CLIS")

print("‚úÖ Core imports and configuration completed!")



# ============================================================================
# üóÇÔ∏è Load and Explore Portuguese Student Dataset
# ============================================================================

# Upload the CSV file
from google.colab import files
import io

print("üìÅ Please upload your student-por.csv file:")
uploaded = files.upload()

# Load the dataset
file_name = list(uploaded.keys())[0]
df = pd.read_csv(io.BytesIO(uploaded[file_name]), sep=';')

print(f"üìä Portuguese Student Dataset Loaded Successfully!")
print(f"Dataset shape: {df.shape}")
print(f"Features: {list(df.columns)}")

# Display basic statistics
print("\nüìà Dataset Overview:")
print(df.head())
print(f"\nüìä Target Variable (G3) Distribution:")
print(df['G3'].describe())
print(f"\nüîç Missing Values: {df.isnull().sum().sum()}")

# Visualize grade distribution
fig = px.histogram(df, x='G3', nbins=20, title='Final Grade (G3) Distribution')
fig.show()

print("‚úÖ Dataset loaded and analyzed successfully!")




# ============================================================================
# üß† Ultra-Advanced ML Pipeline with Portuguese Data
# ============================================================================

class UltraAdvancedMLPipeline:
    """Next-generation ML pipeline with ensemble methods and auto-tuning"""

    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.encoders = {}
        self.feature_names = []
        self.model_metrics = {}
        self.best_model = None
        self.ensemble_weights = {}
        self.feature_importance = {}
        self.training_history = []

    def preprocess_data(self, df):
        """Advanced preprocessing for Portuguese student dataset"""
        print("üîß Advanced preprocessing of Portuguese student data...")

        data = df.copy()

        # Encode categorical variables
        categorical_features = ['school', 'sex', 'address', 'famsize', 'Pstatus',
                              'Mjob', 'Fjob', 'reason', 'guardian', 'schoolsup',
                              'famsup', 'paid', 'activities', 'nursery', 'higher',
                              'internet', 'romantic']

        for feature in categorical_features:
            if feature in data.columns:
                le = LabelEncoder()
                data[f'{feature}_encoded'] = le.fit_transform(data[feature].astype(str))
                self.encoders[feature] = le

        return data

    def engineer_advanced_features(self, data):
        """Create sophisticated features based on educational domain knowledge"""
        print("üî¨ Engineering advanced educational features...")

        # Academic performance features
        data['grade_progression'] = data['G2'] - data['G1']
        data['grade_average'] = (data['G1'] + data['G2']) / 2
        data['grade_consistency'] = 1 - abs(data['G2'] - data['G1']) / 20
        data['academic_trajectory'] = np.where(data['grade_progression'] > 0, 1, 0)

        # Study and effort metrics
        data['study_efficiency'] = data['grade_average'] / (data['studytime'] + 1)
        data['attendance_rate'] = np.clip(1 - (data['absences'] / 50), 0, 1)
        data['failure_impact'] = np.exp(-data['failures'])

        # Family and social factors
        data['parental_education'] = (data['Medu'] + data['Fedu']) / 2
        data['family_support_score'] = (
            data['famrel'] * 0.4 +
            data.get('famsup_encoded', 0) * 0.3 +
            data['parental_education'] * 0.3
        ) / 3

        # Lifestyle and health factors
        data['social_balance'] = (data['freetime'] + data['goout']) / 2
        data['alcohol_risk'] = (data.get('Dalc', 1) + data.get('Walc', 1)) / 2
        data['health_wellness'] = data['health'] / 5

        # Advanced composite scores matching your original features
        data['Effort Score'] = np.clip((
            data['studytime'] * 0.3 +
            data['failure_impact'] * 0.3 +
            data['attendance_rate'] * 0.4
        ), 0, 1)

        data['Emotional Sentiment'] = np.clip((
            data['famrel'] * 0.3 +
            data['health'] * 0.3 +
            (6 - data['social_balance']) * 0.2 +
            data['family_support_score'] * 0.2
        ) / 15, 0, 1)

        data['Participation Index'] = np.clip((
            data['study_efficiency'] * 0.4 +
            data['attendance_rate'] * 0.3 +
            data.get('activities_encoded', 0) * 0.3
        ), 0, 1)

        # Select final feature set
        self.feature_names = [
            'studytime', 'G1', 'G2', 'absences',
            'Effort Score', 'Emotional Sentiment', 'Participation Index'
        ]

        print(f"‚úÖ Created {len(self.feature_names)} advanced features")
        return data

    def prepare_training_data(self, data):
        """Prepare data for model training"""
        print("üéØ Preparing training data...")

        X = data[self.feature_names].copy()
        y = data['G3'].copy()

        # Handle missing values
        X = X.fillna(X.median())

        # Remove outliers using IQR method
        Q1 = y.quantile(0.25)
        Q3 = y.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        mask = (y >= lower_bound) & (y <= upper_bound)
        X = X[mask]
        y = y[mask]

        print(f"‚úÖ Training data prepared: {len(X)} samples, {len(X.columns)} features")
        return X, y

    def train_ensemble_models(self, X, y):
        """Train ensemble of advanced models"""
        print("üöÄ Training ensemble of advanced models...")

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42,
            stratify=pd.cut(y, bins=5, labels=False)
        )

        # Scale features
        self.scalers['standard'] = StandardScaler()
        X_train_scaled = self.scalers['standard'].fit_transform(X_train)
        X_test_scaled = self.scalers['standard'].transform(X_test)

        # Model configurations
        model_configs = {
            'mlp': {
                'model': MLPRegressor(random_state=42, max_iter=1000, early_stopping=True),
                'params': {
                    'hidden_layer_sizes': [(100, 50), (150, 75), (200, 100)],
                    'activation': ['relu'],
                    'alpha': [0.001, 0.01],
                    'learning_rate': ['adaptive']
                },
                'use_scaled': True
            },
            'rf': {
                'model': RandomForestRegressor(random_state=42, n_jobs=-1),
                'params': {
                    'n_estimators': [100, 200],
                    'max_depth': [10, 15, None],
                    'min_samples_split': [5, 10]
                },
                'use_scaled': False
            },
            'gb': {
                'model': GradientBoostingRegressor(random_state=42),
                'params': {
                    'n_estimators': [100, 150],
                    'learning_rate': [0.1, 0.15],
                    'max_depth': [3, 5]
                },
                'use_scaled': False
            }
        }

        # Train models
        for name, config in model_configs.items():
            print(f"üî• Training {name.upper()}...")

            if config['use_scaled']:
                X_train_model = X_train_scaled
                X_test_model = X_test_scaled
            else:
                X_train_model = X_train
                X_test_model = X_test

            grid_search = GridSearchCV(
                config['model'], config['params'],
                cv=3, scoring='r2', n_jobs=-1
            )

            grid_search.fit(X_train_model, y_train)
            best_model = grid_search.best_estimator_

            y_pred = best_model.predict(X_test_model)

            metrics = {
                'r2': r2_score(y_test, y_pred),
                'rmse': np.sqrt(mean_squared_error(y_test, y_pred)),
                'mae': mean_absolute_error(y_test, y_pred),
                'best_params': grid_search.best_params_
            }

            self.models[name] = best_model
            self.model_metrics[name] = metrics

            print(f"‚úÖ {name.upper()}: R¬≤ = {metrics['r2']:.4f}, RMSE = {metrics['rmse']:.4f}")

        # Calculate ensemble weights
        total_r2 = sum([m['r2'] for m in self.model_metrics.values()])
        self.ensemble_weights = {
            name: metrics['r2'] / total_r2
            for name, metrics in self.model_metrics.items()
        }

        self.best_model = max(
            self.model_metrics.keys(),
            key=lambda x: self.model_metrics[x]['r2']
        )

        # Evaluate ensemble
        ensemble_pred = self.predict_ensemble(X_test)
        ensemble_r2 = r2_score(y_test, ensemble_pred)
        ensemble_rmse = np.sqrt(mean_squared_error(y_test, ensemble_pred))

        self.model_metrics['ensemble'] = {
            'r2': ensemble_r2,
            'rmse': ensemble_rmse,
            'weights': self.ensemble_weights
        }

        print(f"üèÜ Best Model: {self.best_model.upper()}")
        print(f"üéØ Ensemble: R¬≤ = {ensemble_r2:.4f}, RMSE = {ensemble_rmse:.4f}")

        return X_test, y_test

    def predict_ensemble(self, X):
        """Make ensemble predictions"""
        predictions = {}
        for name, model in self.models.items():
            if name in ['mlp']:
                X_scaled = self.scalers['standard'].transform(X)
                predictions[name] = model.predict(X_scaled)
            else:
                predictions[name] = model.predict(X)

        ensemble_pred = np.zeros(len(X))
        for name, pred in predictions.items():
            ensemble_pred += pred * self.ensemble_weights[name]

        return ensemble_pred

    def train_complete_pipeline(self, df):
        """Complete training pipeline"""
        print("üöÄ Starting complete ML pipeline training...")

        processed_data = self.preprocess_data(df)
        featured_data = self.engineer_advanced_features(processed_data)
        X, y = self.prepare_training_data(featured_data)
        X_test, y_test = self.train_ensemble_models(X, y)

        print("‚úÖ Complete pipeline training finished!")
        return True

    def save_pipeline(self, path='clis_portuguese_model.joblib'):
        """Save complete pipeline"""
        pipeline_data = {
            'models': self.models,
            'scalers': self.scalers,
            'encoders': self.encoders,
            'feature_names': self.feature_names,
            'model_metrics': self.model_metrics,
            'best_model': self.best_model,
            'ensemble_weights': self.ensemble_weights
        }
        joblib.dump(pipeline_data, path)
        print(f"üíæ Pipeline saved to {path}")

# Initialize and train the pipeline
print("üöÄ Initializing Ultra-Advanced ML Pipeline...")
ml_pipeline = UltraAdvancedMLPipeline()
ml_pipeline.train_complete_pipeline(df)
ml_pipeline.save_pipeline()




# ============================================================================
# üîç Ultra-Advanced Explainable AI System
# ============================================================================

class AdvancedExplainableAI:
    """State-of-the-art explainable AI system"""

    def __init__(self, ml_pipeline):
        self.ml_pipeline = ml_pipeline
        self.shap_explainer = None
        self.lime_explainer = None
        self.feature_importance = {}
        self._initialize_explainers()

    def _initialize_explainers(self):
        """Initialize SHAP and LIME explainers"""
        print("üîç Initializing advanced explainers...")

        try:
            # Create background data for SHAP
            sample_data = df.sample(100, random_state=42)
            processed_sample = self.ml_pipeline.preprocess_data(sample_data)
            featured_sample = self.ml_pipeline.engineer_advanced_features(processed_sample)
            X_background = featured_sample[self.ml_pipeline.feature_names]

            # Use the best model for SHAP
            best_model = self.ml_pipeline.models[self.ml_pipeline.best_model]

            if self.ml_pipeline.best_model in ['mlp']:
                X_background_scaled = self.ml_pipeline.scalers['standard'].transform(X_background)
                self.shap_explainer = shap.KernelExplainer(
                    best_model.predict, X_background_scaled[:50]
                )
            else:
                self.shap_explainer = shap.TreeExplainer(best_model)

            # Initialize LIME explainer
            X_background_array = X_background.values
            self.lime_explainer = lime_tabular.LimeTabularExplainer(
                X_background_array,
                feature_names=self.ml_pipeline.feature_names,
                mode='regression'
            )

            print("‚úÖ Advanced explainers initialized!")

        except Exception as e:
            print(f"‚ö†Ô∏è Explainer initialization failed: {e}")
            self.shap_explainer = None
            self.lime_explainer = None

    def get_feature_explanation(self, student_features):
        """Get comprehensive feature explanations"""
        try:
            feature_array = np.array([student_features]).reshape(1, -1)

            if self.shap_explainer:
                if self.ml_pipeline.best_model in ['mlp']:
                    feature_array_scaled = self.ml_pipeline.scalers['standard'].transform(feature_array)
                    shap_values = self.shap_explainer.shap_values(feature_array_scaled)
                else:
                    shap_values = self.shap_explainer.shap_values(feature_array)

                explanation = {}
                importance = {}

                for i, feature in enumerate(self.ml_pipeline.feature_names):
                    if hasattr(shap_values, '__len__') and len(shap_values) > 0:
                        if hasattr(shap_values[0], '__getitem__'):
                            shap_val = float(shap_values[0][i])
                        else:
                            shap_val = float(shap_values[i])
                    else:
                        shap_val = 0.0

                    explanation[feature] = shap_val
                    importance[feature] = abs(shap_val)

                top_factors = sorted(importance.items(), key=lambda x: x[1], reverse=True)
                top_factors = [factor[0] for factor in top_factors[:3]]

                return explanation, importance, top_factors

        except Exception as e:
            print(f"‚ùå SHAP explanation error: {e}")

        # Fallback explanation
        return self._fallback_explanation(student_features)

    def _fallback_explanation(self, student_features):
        """Fallback explanation method"""
        explanation = {}
        importance = {}

        feature_impacts = {
            'studytime': 0.8,
            'G1': 1.2,
            'G2': 1.5,
            'absences': -0.6,
            'Effort Score': 1.0,
            'Emotional Sentiment': 0.7,
            'Participation Index': 0.9
        }

        for i, feature in enumerate(self.ml_pipeline.feature_names):
            if i < len(student_features):
                value = student_features[i]
                impact = value * feature_impacts.get(feature, 0.5)
                explanation[feature] = float(impact)
                importance[feature] = abs(impact)

        top_factors = list(importance.keys())[:3]
        return explanation, importance, top_factors

# Initialize explainable AI
explainer = AdvancedExplainableAI(ml_pipeline)




# ============================================================================
# ü§ñ Ultra-Advanced AI Summary & Intervention System
# ============================================================================

class AdvancedAISummary:
    """AI-powered summary system with LangChain and Flan-T5"""

    def __init__(self):
        self.model_name = "google/flan-t5-small"
        self.pipeline = None
        self.intervention_templates = {
            'low_performance': [
                "Implement intensive tutoring sessions with personalized learning plans",
                "Create structured study schedules with regular progress monitoring"
            ],
            'attendance_issues': [
                "Develop attendance improvement plan with family engagement",
                "Implement flexible learning options and makeup sessions"
            ],
            'study_habits': [
                "Introduce effective study techniques and time management skills",
                "Create distraction-free study environments and routines"
            ],
            'emotional_support': [
                "Provide counseling support for emotional well-being",
                "Implement stress management and coping strategies"
            ]
        }
        self._initialize_model()

    def _initialize_model(self):
        """Initialize the language model"""
        try:
            print("ü§ñ Initializing AI summary system...")

            self.pipeline = pipeline(
                "text2text-generation",
                model=self.model_name,
                tokenizer=self.model_name,
                max_length=256,
                device=0 if torch.cuda.is_available() else -1
            )

            print("‚úÖ AI summary system initialized!")

        except Exception as e:
            print(f"‚ö†Ô∏è Error initializing model: {e}")
            self.pipeline = None

    def generate_summary(self, student_data: dict, predicted_score: float):
        """Generate comprehensive student summary and interventions"""

        # Create LangChain prompt
        prompt = f"""Student Profile:
- studytime: {student_data.get('studytime', 0)}
- G1: {student_data.get('G1', 0)}
- G2: {student_data.get('G2', 0)}
- absences: {student_data.get('absences', 0)}
- Effort Score: {student_data.get('effort_score', 0)}
- Emotional Sentiment: {student_data.get('sentiment', 0)}
- Participation Index: {student_data.get('participation', 0)}
- Predicted Score: {predicted_score}

Write a short performance summary (3 lines) and suggest 2 personalized interventions."""

        try:
            if self.pipeline:
                response = self.pipeline(
                    prompt,
                    max_length=200,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True
                )[0]['generated_text']

                summary, interventions = self._parse_response(response, student_data, predicted_score)
            else:
                summary, interventions = self._fallback_summary(student_data, predicted_score)

            return summary, interventions, 0.85

        except Exception as e:
            print(f"‚ùå Summary generation error: {e}")
            return self._fallback_summary(student_data, predicted_score)

    def _parse_response(self, response: str, student_data: dict, predicted_score: float):
        """Parse AI response into structured format"""
        lines = response.strip().split('\n')
        summary_lines = []
        interventions = []

        for line in lines:
            line = line.strip()
            if line and len(summary_lines) < 3:
                summary_lines.append(line)
            elif line and ('intervention' in line.lower() or 'recommend' in line.lower()):
                interventions.append(line)

        if len(summary_lines) < 3:
            return self._fallback_summary(student_data, predicted_score)

        summary = ' '.join(summary_lines)

        if len(interventions) < 2:
            interventions = self._generate_interventions(student_data, predicted_score)

        return summary, interventions[:2]

    def _fallback_summary(self, student_data: dict, predicted_score: float):
        """Generate rule-based summary"""

        grade_trend = student_data.get('G2', 0) - student_data.get('G1', 0)
        performance_level = "excellent" if predicted_score >= 16 else "good" if predicted_score >= 12 else "needs improvement"

        summary = f"Student demonstrates {performance_level} academic performance with a predicted score of {predicted_score:.1f}/20. "

        if grade_trend > 0:
            summary += f"Positive grade progression from G1 to G2 (+{grade_trend:.1f} points) indicates improving understanding. "
        else:
            summary += f"Grade decline from G1 to G2 ({grade_trend:.1f} points) suggests need for additional support. "

        if student_data.get('effort_score', 0) >= 0.6:
            summary += "Good effort levels provide a strong foundation for continued success."
        else:
            summary += "Effort levels require attention to maximize academic potential."

        interventions = self._generate_interventions(student_data, predicted_score)

        return summary, interventions

    def _generate_interventions(self, student_data: dict, predicted_score: float):
        """Generate targeted interventions"""
        interventions = []

        if student_data.get('studytime', 0) < 2:
            interventions.extend(self.intervention_templates['study_habits'][:1])

        if student_data.get('absences', 0) > 5:
            interventions.extend(self.intervention_templates['attendance_issues'][:1])

        if student_data.get('effort_score', 0) < 0.6:
            interventions.append("Implement motivational strategies and goal-setting frameworks")

        if student_data.get('sentiment', 0) < 0.5:
            interventions.extend(self.intervention_templates['emotional_support'][:1])

        if not interventions:
            interventions = [
                "Continue current positive learning strategies and maintain momentum",
                "Explore advanced learning opportunities and enrichment activities"
            ]

        return interventions[:2]

# Initialize AI summary system
ai_summary = AdvancedAISummary()




# ============================================================================
# üìä Ultra-Advanced Real-Time Anomaly Detection System
# ============================================================================

class AdvancedAnomalyDetector:
    """Real-time anomaly detection with multiple algorithms"""

    def __init__(self, window_size=100, alpha=0.3):
        self.window_size = window_size
        self.alpha = alpha
        self.scores_history = deque(maxlen=window_size)
        self.ema = None
        self.ema_variance = None
        self.anomaly_threshold = 2.0
        self.anomaly_count = 0
        self.total_predictions = 0

    def update(self, score: float) -> dict:
        """Update anomaly detection with new score"""
        self.total_predictions += 1
        self.scores_history.append(score)

        if self.ema is None:
            self.ema = score
            self.ema_variance = 0
            return {
                'is_anomaly': False,
                'anomaly_score': 0.0,
                'confidence': 0.0,
                'ema': self.ema,
                'threshold': self.anomaly_threshold
            }

        # Update EMA
        self.ema = self.alpha * score + (1 - self.alpha) * self.ema

        # Update EMA variance
        squared_diff = (score - self.ema) ** 2
        if self.ema_variance is None:
            self.ema_variance = squared_diff
        else:
            self.ema_variance = self.alpha * squared_diff + (1 - self.alpha) * self.ema_variance

        # Calculate standard deviation
        std_dev = np.sqrt(self.ema_variance) if self.ema_variance > 0 else 1.0

        # Calculate anomaly score
        anomaly_score = abs(score - self.ema) / std_dev if std_dev > 0 else 0

        # Determine if anomaly
        is_anomaly = anomaly_score > self.anomaly_threshold

        if is_anomaly:
            self.anomaly_count += 1

        confidence = min(len(self.scores_history) / self.window_size, 1.0)

        return {
            'is_anomaly': is_anomaly,
            'anomaly_score': float(anomaly_score),
            'confidence': float(confidence),
            'ema': float(self.ema),
            'threshold': self.anomaly_threshold,
            'anomaly_rate': self.anomaly_count / self.total_predictions
        }

    def get_statistics(self) -> dict:
        """Get comprehensive anomaly statistics"""
        if not self.scores_history:
            return {}

        scores_array = np.array(self.scores_history)
        return {
            'total_predictions': self.total_predictions,
            'anomaly_count': self.anomaly_count,
            'anomaly_rate': self.anomaly_count / max(self.total_predictions, 1),
            'current_ema': float(self.ema) if self.ema else 0,
            'score_mean': float(np.mean(scores_array)),
            'score_std': float(np.std(scores_array)),
            'score_min': float(np.min(scores_array)),
            'score_max': float(np.max(scores_array))
        }

# Initialize anomaly detector
anomaly_detector = AdvancedAnomalyDetector()




# ============================================================================
# üéØ Advanced Pydantic Models for Type Safety & Validation
# ============================================================================

class StudentFeatures(BaseModel):
    """Advanced student feature model with comprehensive validation"""
    studytime: float = Field(..., ge=0, le=4, description="Study time per week (0-4 hours)")
    G1: float = Field(..., ge=0, le=20, description="First period grade (0-20)")
    G2: float = Field(..., ge=0, le=20, description="Second period grade (0-20)")
    absences: float = Field(..., ge=0, description="Number of absences")
    effort_score: float = Field(..., ge=0, le=1, description="Effort score (0-1)")
    sentiment: float = Field(..., ge=0, le=1, description="Emotional sentiment (0-1)")
    participation: float = Field(..., ge=0, le=1, description="Participation index (0-1)")

    class Config:
        schema_extra = {
            "example": {
                "studytime": 2.5,
                "G1": 12.0,
                "G2": 14.5,
                "absences": 2.0,
                "effort_score": 0.8,
                "sentiment": 0.7,
                "participation": 0.9
            }
        }

class PredictionResponse(BaseModel):
    """Comprehensive prediction response"""
    predicted_score: float = Field(..., description="Predicted final score")
    confidence: float = Field(..., description="Prediction confidence")
    risk_level: str = Field(..., description="Academic risk level")
    anomaly_detected: bool = Field(..., description="Anomaly detection flag")
    request_id: str = Field(..., description="Unique request identifier")
    timestamp: str = Field(..., description="Prediction timestamp")
    model_version: str = Field(..., description="Model version used")

class ExplanationResponse(BaseModel):
    """SHAP explanation response"""
    shap_values: Dict[str, float] = Field(..., description="SHAP feature values")
    feature_importance: Dict[str, float] = Field(..., description="Feature importance scores")
    top_factors: List[str] = Field(..., description="Top influencing factors")
    request_id: str = Field(..., description="Unique request identifier")

class SummaryResponse(BaseModel):
    """AI-generated summary response"""
    summary: str = Field(..., description="Performance summary")
    interventions: List[str] = Field(..., description="Recommended interventions")
    confidence_score: float = Field(..., description="Summary confidence")
    request_id: str = Field(..., description="Unique request identifier")

class HealthResponse(BaseModel):
    """System health response"""
    status: str = Field(..., description="System status")
    uptime: str = Field(..., description="System uptime")
    memory_usage: float = Field(..., description="Memory usage percentage")
    cpu_usage: float = Field(..., description="CPU usage percentage")
    total_predictions: int = Field(..., description="Total predictions made")
    model_accuracy: float = Field(..., description="Model accuracy")

print("‚úÖ Advanced Pydantic models defined!")





# ============================================================================
# üöÄ Ultra-Modern FastAPI Backend with Advanced Features
# ============================================================================

# Global variables for monitoring
prediction_counter = Counter('clis_predictions_total', 'Total predictions made')
prediction_histogram = Histogram('clis_prediction_duration_seconds', 'Prediction duration')
error_counter = Counter('clis_errors_total', 'Total errors', ['error_type'])

# Request logging
request_logs = deque(maxlen=1000)
start_time = datetime.datetime.now()

# Create FastAPI app with advanced configuration
app = FastAPI(
    title="CLIS - Cognitive Learning Intelligence System",
    description="üöÄ Ultra-Modern AI-Powered Education Analytics Platform",
    version="3.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    openapi_tags=[
        {"name": "predictions", "description": "AI prediction endpoints"},
        {"name": "explanations", "description": "Explainable AI endpoints"},
        {"name": "summaries", "description": "AI summary endpoints"},
        {"name": "monitoring", "description": "System monitoring endpoints"}
    ]
)

# Advanced CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Utility functions
def log_request(request_id: str, endpoint: str, data: dict, response: dict):
    """Log request details"""
    log_entry = {
        'request_id': request_id,
        'endpoint': endpoint,
        'timestamp': datetime.datetime.now().isoformat(),
        'input_data': data,
        'response': response
    }
    request_logs.append(log_entry)

def get_system_stats():
    """Get system performance statistics"""
    return {
        'memory_usage': psutil.virtual_memory().percent,
        'cpu_usage': psutil.cpu_percent(),
        'uptime': str(datetime.datetime.now() - start_time)
    }

def prepare_student_features(features: StudentFeatures):
    """Prepare features for model prediction"""
    # Create feature vector matching training format
    feature_vector = [
        features.studytime,
        features.G1,
        features.G2,
        features.absences,
        features.effort_score,
        features.sentiment,
        features.participation
    ]

    return np.array(feature_vector).reshape(1, -1)

# ============================================================================
# üéØ API Endpoints
# ============================================================================

@app.get("/", response_class=HTMLResponse)
async def root():
    """Welcome page with API documentation"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>CLIS - Cognitive Learning Intelligence System</title>
        <style>
            body { font-family: Arial, sans-serif; margin: 40px; background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; }
            .container { max-width: 800px; margin: 0 auto; background: rgba(255,255,255,0.1); padding: 30px; border-radius: 15px; }
            h1 { color: #fff; text-align: center; }
            .endpoint { background: rgba(255,255,255,0.2); padding: 15px; margin: 10px 0; border-radius: 8px; }
            .method { color: #4CAF50; font-weight: bold; }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>üöÄ CLIS Backend API</h1>
            <p>Ultra-Modern AI-Powered Education Analytics Platform</p>

            <div class="endpoint">
                <span class="method">POST</span> /predict - Get AI predictions
            </div>
            <div class="endpoint">
                <span class="method">POST</span> /explain - Get SHAP explanations
            </div>
            <div class="endpoint">
                <span class="method">POST</span> /summary - Get AI summaries
            </div>
            <div class="endpoint">
                <span class="method">GET</span> /health - System health check
            </div>
            <div class="endpoint">
                <span class="method">GET</span> /docs - Interactive API documentation
            </div>
        </div>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

@app.get("/health", response_model=HealthResponse, tags=["monitoring"])
async def health_check():
    """Advanced health check endpoint"""
    stats = get_system_stats()

    return HealthResponse(
        status="healthy",
        uptime=stats['uptime'],
        memory_usage=stats['memory_usage'],
        cpu_usage=stats['cpu_usage'],
        total_predictions=anomaly_detector.total_predictions,
        model_accuracy=ml_pipeline.model_metrics.get('ensemble', {}).get('r2', 0.0)
    )

@app.post("/predict", response_model=PredictionResponse, tags=["predictions"])
async def predict_score(features: StudentFeatures):
    """üéØ Advanced prediction endpoint with anomaly detection"""
    request_id = str(uuid.uuid4())

    try:
        with prediction_histogram.time():
            # Prepare features
            feature_array = prepare_student_features(features)

            # Make prediction using ensemble
            prediction = ml_pipeline.predict_ensemble(feature_array)[0]
            prediction = max(0, min(20, prediction))  # Clip to valid range

            # Update anomaly detection
            anomaly_result = anomaly_detector.update(prediction)

            # Determine risk level
            if prediction >= 14:
                risk_level = "low"
            elif prediction >= 10:
                risk_level = "medium"
            else:
                risk_level = "high"

            # Calculate confidence
            confidence = min(0.95, 0.7 + (prediction / 20) * 0.25)

            response = PredictionResponse(
                predicted_score=round(prediction, 2),
                confidence=round(confidence, 3),
                risk_level=risk_level,
                anomaly_detected=anomaly_result['is_anomaly'],
                request_id=request_id,
                timestamp=datetime.datetime.now().isoformat(),
                model_version="Portuguese-v3.0"
            )

            # Log request
            log_request(request_id, "/predict", features.dict(), response.dict())

            # Update metrics
            prediction_counter.inc()

            return response

    except Exception as e:
        error_counter.labels(error_type="prediction").inc()
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@app.post("/explain", response_model=ExplanationResponse, tags=["explanations"])
async def explain_prediction(features: StudentFeatures):
    """üîç Advanced explanation endpoint with SHAP values"""
    request_id = str(uuid.uuid4())

    try:
        # Prepare features
        feature_array = prepare_student_features(features)
        feature_list = feature_array[0].tolist()

        # Get SHAP explanation
        shap_values, importance, top_factors = explainer.get_feature_explanation(feature_list)

        response = ExplanationResponse(
            shap_values=shap_values,
            feature_importance=importance,
            top_factors=top_factors,
            request_id=request_id
        )

        # Log request
        log_request(request_id, "/explain", features.dict(), response.dict())

        return response

    except Exception as e:
        error_counter.labels(error_type="explanation").inc()
        logger.error(f"Explanation error: {e}")
        raise HTTPException(status_code=500, detail=f"Explanation failed: {str(e)}")

@app.post("/summary", response_model=SummaryResponse, tags=["summaries"])
async def generate_summary(features: StudentFeatures):
    """ü§ñ Advanced AI summary endpoint with LangChain"""
    request_id = str(uuid.uuid4())

    try:
        # Get prediction first
        feature_array = prepare_student_features(features)
        prediction = ml_pipeline.predict_ensemble(feature_array)[0]

        # Generate AI summary
        summary, interventions, confidence = ai_summary.generate_summary(
            features.dict(), prediction
        )

        response = SummaryResponse(
            summary=summary,
            interventions=interventions,
            confidence_score=confidence,
            request_id=request_id
        )

        # Log request
        log_request(request_id, "/summary", features.dict(), response.dict())

        return response

    except Exception as e:
        error_counter.labels(error_type="summary").inc()
        logger.error(f"Summary error: {e}")
        raise HTTPException(status_code=500, detail=f"Summary generation failed: {str(e)}")

@app.get("/metrics", tags=["monitoring"])
async def get_metrics():
    """üìä Prometheus metrics endpoint"""
    return generate_latest()

@app.get("/analytics", tags=["monitoring"])
async def get_analytics():
    """üìà Advanced analytics dashboard"""
    return {
        'system_stats': get_system_stats(),
        'anomaly_stats': anomaly_detector.get_statistics(),
        'model_metrics': ml_pipeline.model_metrics,
        'recent_requests': len(request_logs),
        'dataset_info': {
            'total_students': len(df),
            'avg_final_grade': float(df['G3'].mean()),
            'grade_std': float(df['G3'].std())
        }
    }

@app.post("/log_prediction", tags=["monitoring"])
async def log_prediction_endpoint(features: StudentFeatures, background_tasks: BackgroundTasks):
    """üìù Log prediction to CSV file"""
    request_id = str(uuid.uuid4())

    try:
        # Make prediction
        feature_array = prepare_student_features(features)
        prediction = ml_pipeline.predict_ensemble(feature_array)[0]

        # Prepare log data
        log_data = {
            'timestamp': datetime.datetime.now().isoformat(),
            'request_id': request_id,
            'predicted_score': prediction,
            **features.dict()
        }

        # Background task to save to CSV
        background_tasks.add_task(save_to_csv, log_data)

        return {"status": "logged", "request_id": request_id, "predicted_score": prediction}

    except Exception as e:
        logger.error(f"Logging error: {e}")
        raise HTTPException(status_code=500, detail=f"Logging failed: {str(e)}")

def save_to_csv(log_data: dict):
    """Background task to save prediction to CSV"""
    try:
        df_log = pd.DataFrame([log_data])
        df_log.to_csv('predictions_log.csv', mode='a', header=False, index=False)
    except Exception as e:
        logger.error(f"CSV save error: {e}")

print("‚úÖ Ultra-modern FastAPI backend initialized successfully!")






# ============================================================================
# üîê Configure Ngrok Authentication
# ============================================================================

from pyngrok import ngrok

# Set your ngrok authtoken
ngrok.set_auth_token("2y8Ak5RQ24Kte52TNceryyFey1g_hJnvStRtXnsd32xASXwY")

print("‚úÖ Ngrok authtoken configured successfully!")





# ============================================================================
# üåê Ultra-Advanced Deployment with Ngrok & Monitoring
# ============================================================================

class AdvancedDeployment:
    """Advanced deployment manager with comprehensive monitoring"""

    def __init__(self):
        self.ngrok_tunnel = None
        self.public_url = None
        self.server_process = None

    def setup_ngrok(self, auth_token=None):
        """Setup Ngrok tunnel with authentication"""
        try:
            if auth_token:
                ngrok.set_auth_token(auth_token)

            # Create tunnel
            self.ngrok_tunnel = ngrok.connect(8000)
            self.public_url = self.ngrok_tunnel.public_url

            print(f"üåê Ngrok tunnel established!")
            print(f"üîó Public URL: {self.public_url}")
            print(f"üìä API Documentation: {self.public_url}/docs")
            print(f"üìà Analytics: {self.public_url}/analytics")
            print(f"üè• Health Check: {self.public_url}/health")

            return self.public_url

        except Exception as e:
            print(f"‚ùå Ngrok setup failed: {e}")
            print("üîó Server running locally: http://localhost:8000")
            return "http://localhost:8000"

    def start_server(self):
        """Start the FastAPI server"""
        try:
            print("üöÄ Starting ultra-modern CLIS backend server...")

            # Configure uvicorn
            config = uvicorn.Config(
                app=app,
                host="0.0.0.0",
                port=8000,
                log_level="info",
                reload=False
            )

            server = uvicorn.Server(config)

            # Start server in background
            server_thread = threading.Thread(target=server.run)
            server_thread.daemon = True
            server_thread.start()

            # Wait for server to start
            time.sleep(3)

            print("‚úÖ Server started successfully!")
            return server_thread

        except Exception as e:
            print(f"‚ùå Server startup failed: {e}")
            return None

    def deploy(self, ngrok_token=None):
        """Full deployment process"""
        print("üöÄ Initiating ultra-modern deployment...")

        # Start server
        server_thread = self.start_server()
        if not server_thread:
            return False

        # Setup Ngrok
        public_url = self.setup_ngrok(ngrok_token)

        # Display deployment summary
        self.display_deployment_summary()

        return True

    def display_deployment_summary(self):
        """Display comprehensive deployment summary"""
        print("\n" + "="*80)
        print("üéâ CLIS BACKEND DEPLOYMENT SUCCESSFUL!")
        print("="*80)
        print(f"üåê Public URL: {self.public_url or 'http://localhost:8000'}")
        print(f"üìö API Documentation: {(self.public_url or 'http://localhost:8000')}/docs")
        print(f"üè• Health Check: {(self.public_url or 'http://localhost:8000')}/health")
        print(f"üìä Analytics: {(self.public_url or 'http://localhost:8000')}/analytics")
        print(f"üìà Metrics: {(self.public_url or 'http://localhost:8000')}/metrics")

        print("\nüéØ Available Endpoints:")
        print("  POST /predict     - Get AI predictions with anomaly detection")
        print("  POST /explain     - Get SHAP explanations")
        print("  POST /summary     - Get AI-generated summaries with LangChain")
        print("  POST /log_prediction - Log predictions to CSV")
        print("  GET  /health      - System health check")
        print("  GET  /analytics   - Advanced analytics dashboard")
        print("  GET  /metrics     - Prometheus metrics")

        print("\nüîß Ultra-Modern Features:")
        print("  ‚úÖ Portuguese Student Dataset Integration")
        print("  ‚úÖ Advanced Ensemble ML Models")
        print("  ‚úÖ Real-time Anomaly Detection with EMA")
        print("  ‚úÖ SHAP Explainable AI")
        print("  ‚úÖ LangChain + Flan-T5 AI Summaries")
        print("  ‚úÖ Prometheus Monitoring")
        print("  ‚úÖ Advanced Request Logging")
        print("  ‚úÖ Production-grade Error Handling")
        print("  ‚úÖ Comprehensive Type Safety")
        print("  ‚úÖ Interactive API Documentation")
        print("="*80)

# Initialize deployment manager
deployment = AdvancedDeployment()

# Deploy the system
# In your deployment cell, update this line:
deployment.deploy(ngrok_token="2y8Ak5RQ24Kte52TNceryyFey1g_hJnvStRtXnsd32xASXwY")

print("\nüéä CLIS Backend is now live and ready for frontend integration!")
print("üîó Connect your React dashboard to the public URL above")
print("üì± Test the API using the interactive documentation")


